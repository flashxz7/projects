{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df58840b-ad08-4681-96ae-fd4e2eb7d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LITERARY SEMANTIC FRAME ANALYZER\n",
      "================================================================================\n",
      "\n",
      "Processing 9184 sentences...\n",
      "  Progress: 0/9184 sentences\n",
      "  Progress: 1000/9184 sentences\n",
      "  Progress: 2000/9184 sentences\n",
      "  Progress: 3000/9184 sentences\n",
      "  Progress: 4000/9184 sentences\n",
      "  Progress: 5000/9184 sentences\n",
      "  Progress: 6000/9184 sentences\n",
      "  Progress: 7000/9184 sentences\n",
      "  Progress: 8000/9184 sentences\n",
      "  Progress: 9000/9184 sentences\n",
      "Processing complete\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Book: Moby Dick.txt\n",
      "Total Sentences: 9,184\n",
      "Sentences with Frames Detected: 512\n",
      "Detection Rate: 5.57%\n",
      "\n",
      "What this means: Out of every 100 sentences, about 5 contain revenge, hunting, or travel scenes.\n",
      "\n",
      "BREAKDOWN BY SCENE TYPE:\n",
      "  Travel: 255 instances (49.8% of all detected frames)\n",
      "  Hunting: 214 instances (41.8% of all detected frames)\n",
      "  Revenge: 43 instances (8.4% of all detected frames)\n",
      "\n",
      "================================================================================\n",
      "SENTIMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Revenge scenes:\n",
      "  Average sentiment: -0.081\n",
      "  Positive: 15, Negative: 21, Neutral: 7\n",
      "\n",
      "Hunting scenes:\n",
      "  Average sentiment: -0.106\n",
      "  Positive: 83, Negative: 104, Neutral: 27\n",
      "\n",
      "Travel scenes:\n",
      "  Average sentiment: 0.051\n",
      "  Positive: 108, Negative: 85, Neutral: 62\n",
      "\n",
      "================================================================================\n",
      "TEMPORAL DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Frame occurrences across the book (divided into 10 equal segments):\n",
      "\n",
      "Revenge:\n",
      "  Section  1: |****************** 5\n",
      "  Section  2: |********************* 6\n",
      "  Section  3: |********** 3\n",
      "  Section  4: |***************************** 8\n",
      "  Section  5: |**************************************** 11\n",
      "  Section  6: |********** 3\n",
      "  Section  7: |********** 3\n",
      "  Section  8: | 0\n",
      "  Section  9: |******* 2\n",
      "  Section 10: |******* 2\n",
      "\n",
      "Hunting:\n",
      "  Section  1: |************** 13\n",
      "  Section  2: |************ 12\n",
      "  Section  3: |****************** 17\n",
      "  Section  4: |******************************** 30\n",
      "  Section  5: |****************************** 28\n",
      "  Section  6: |********************* 20\n",
      "  Section  7: |**************************************** 37\n",
      "  Section  8: |********************** 21\n",
      "  Section  9: |******** 8\n",
      "  Section 10: |****************************** 28\n",
      "\n",
      "Travel:\n",
      "  Section  1: |**************************** 26\n",
      "  Section  2: |************************************** 36\n",
      "  Section  3: |**************************************** 37\n",
      "  Section  4: |********************* 20\n",
      "  Section  5: |************************************ 34\n",
      "  Section  6: |*************** 14\n",
      "  Section  7: |******************* 18\n",
      "  Section  8: |******************** 19\n",
      "  Section  9: |******************* 18\n",
      "  Section 10: |*********************************** 33\n",
      "\n",
      "================================================================================\n",
      "VOCABULARY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Most distinctive words by frame type:\n",
      "\n",
      "Revenge: ahab, place, revenge, settle, settled, ship, time, vengeance\n",
      "Hunting: ahab, boat, chase, kill, killed, man, whale, whales\n",
      "Travel: captain, man, sail, sailed, sea, ship, voyage, whale\n",
      "\n",
      "================================================================================\n",
      "WORD FREQUENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Most frequent words in the text:\n",
      "\n",
      "  whale           ======================================== 1,073\n",
      "  one             ================================= 900\n",
      "  like            ===================== 576\n",
      "  upon            ===================== 567\n",
      "  ahab            ================== 494\n",
      "  man             ================== 483\n",
      "  ship            ================= 460\n",
      "  ye              ================ 456\n",
      "  old             ================ 443\n",
      "  would           =============== 429\n",
      "  sea             ============= 375\n",
      "  though          ============= 365\n",
      "  yet             ============ 337\n",
      "  time            ============ 322\n",
      "  captain         =========== 320\n",
      "  long            =========== 312\n",
      "  chapter         =========== 308\n",
      "  still           =========== 308\n",
      "  great           =========== 305\n",
      "  said            =========== 298\n",
      "\n",
      "Total words: 105,195\n",
      "Unique words: 16,438\n",
      "Vocabulary richness: 0.1563\n",
      "\n",
      "================================================================================\n",
      "PROBABILISTIC SENTENCE GENERATION\n",
      "================================================================================\n",
      "\n",
      "Generated sentence based on bigram probabilities:\n",
      "\n",
      "\"Lord save thee job profit reason world yet tell sperm whale baleen similitude.\"\n",
      "\n",
      "Note: This sentence was generated by analyzing which words commonly\n",
      "follow each other in the text. It reflects the statistical patterns\n",
      "of the book's language but may not be grammatically perfect.\n",
      "\n",
      "================================================================================\n",
      "SAMPLE DETECTIONS (showing 10 of 512 total)\n",
      "================================================================================\n",
      "\n",
      "Example 1 (Sentence 152)\n",
      "FRAME TYPE: Hunting\n",
      "TRIGGER WORD: 'kill'\n",
      "\n",
      "SENTENCE:\n",
      "Stubb and Flask kill a Right Whale; and Then Have a Talk\n",
      "over Him.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Instrument: Flask\n",
      "  Location: Stubb\n",
      "\n",
      "Example 2 (Sentence 321)\n",
      "FRAME TYPE: Revenge\n",
      "TRIGGER WORD: 'punish'\n",
      "\n",
      "SENTENCE:\n",
      "“In that day, the Lord with his sore, and great, and strong sword,\n",
      "  shall punish Leviathan the piercing serpent, even Leviathan that\n",
      "  crooked serpent; and he shall slay the dragon that is in the sea.”\n",
      "  —_Isaiah_.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Avenger: Leviathan\n",
      "  Offender: Leviathan\n",
      "\n",
      "Example 3 (Sentence 329)\n",
      "FRAME TYPE: Hunting\n",
      "TRIGGER WORD: 'killed'\n",
      "\n",
      "SENTENCE:\n",
      "He\n",
      "  said that he was one of six who had killed sixty in two days.”\n",
      "  —_Other or Other’s verbal narrative taken down from his mouth by King\n",
      "  Alfred, A.D._ 890.\n",
      "\n",
      "(No frame elements extracted)\n",
      "\n",
      "Example 4 (Sentence 357)\n",
      "FRAME TYPE: Travel\n",
      "TRIGGER WORD: 'sail'\n",
      "\n",
      "SENTENCE:\n",
      "“We set sail from the Elbe, wind N.E.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Traveler: Elbe\n",
      "\n",
      "Example 5 (Sentence 361)\n",
      "FRAME TYPE: Travel\n",
      "TRIGGER WORD: 'Voyage'\n",
      "\n",
      "SENTENCE:\n",
      "I was told of a whale taken near Shetland, that had above a\n",
      "  barrel of herrings in his belly.... One of our harpooneers told me\n",
      "  that he caught once a whale in Spitzbergen that was white all over.”\n",
      "  —_A Voyage to Greenland, A.D._ 1671.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Source: Spitzbergen\n",
      "  Goal: Spitzbergen\n",
      "  Path: Spitzbergen\n",
      "\n",
      "Example 6 (Sentence 365)\n",
      "FRAME TYPE: Hunting\n",
      "TRIGGER WORD: 'kill'\n",
      "\n",
      "SENTENCE:\n",
      "“Myself have agreed to try whether I can master and kill this\n",
      "  Sperma-ceti whale, for I could never hear of any of that sort that\n",
      "  was killed by any man, such is his fierceness and swiftness.”\n",
      "  —_Richard Strafford’s Letter from the Bermudas.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Instrument: can\n",
      "\n",
      "Example 7 (Sentence 371)\n",
      "FRAME TYPE: Travel\n",
      "TRIGGER WORD: 'Voyage'\n",
      "\n",
      "SENTENCE:\n",
      "“We saw also abundance of large whales, there being more in those\n",
      "  southern seas, as I may say, by a hundred to one; than we have to the\n",
      "  northward of us.” —_Captain Cowley’s Voyage round the Globe, A.D._\n",
      "  1729.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Traveler: Cowley\n",
      "  Source: Globe\n",
      "  Goal: Globe\n",
      "  Path: Globe\n",
      "\n",
      "Example 8 (Sentence 379)\n",
      "FRAME TYPE: Hunting\n",
      "TRIGGER WORD: 'killed'\n",
      "\n",
      "SENTENCE:\n",
      "“In the afternoon we saw what was supposed to be a rock, but it was\n",
      "  found to be a dead whale, which some Asiatics had killed, and were\n",
      "  then towing ashore.\n",
      "\n",
      "(No frame elements extracted)\n",
      "\n",
      "Example 9 (Sentence 382)\n",
      "FRAME TYPE: Travel\n",
      "TRIGGER WORD: 'Voyage'\n",
      "\n",
      "SENTENCE:\n",
      "They stand in so\n",
      "  great dread of some of them, that when out at sea they are afraid to\n",
      "  mention even their names, and carry dung, lime-stone, juniper-wood,\n",
      "  and some other articles of the same nature in their boats, in order\n",
      "  to terrify and prevent their too near approach.” —_Uno Von Troil’s\n",
      "  Letters on Banks’s and Solander’s Voyage to Iceland in_ 1772.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Traveler: Solander\n",
      "  Source: Iceland\n",
      "  Goal: Iceland\n",
      "  Path: Iceland\n",
      "\n",
      "Example 10 (Sentence 396)\n",
      "FRAME TYPE: Travel\n",
      "TRIGGER WORD: 'Voyage'\n",
      "\n",
      "SENTENCE:\n",
      "“In 40 degrees south, we saw Spermacetti Whales, but did not take any\n",
      "  till the first of May, the sea being then covered with them.”\n",
      "  —_Colnett’s Voyage for the Purpose of Extending the Spermaceti Whale\n",
      "  Fishery_.\n",
      "\n",
      "DETECTED ROLES:\n",
      "  Traveler: Spermacetti Whales\n",
      "\n",
      "\n",
      "Exported 512 detections to 'moby_dick_frame_analysis.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk, Tree\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "class LiteraryFrameAnalyzer:\n",
    "    def __init__(self, corpus_path, gazetteer_path=None):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.gazetteer = {}\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "        self.frame_definitions = {\n",
    "            \"Revenge\": {\n",
    "                \"lexical_units\": [\"revenge\", \"avenge\", \"retaliate\", \"retribution\", \"vengeance\", \n",
    "                                 \"reprisal\", \"payback\", \"settle\", \"punish\"],\n",
    "                \"frame_elements\": {\n",
    "                    \"Avenger\": [\"PERSON\", \"ORGANIZATION\"],\n",
    "                    \"Offender\": [\"PERSON\", \"ORGANIZATION\"],\n",
    "                    \"Injury\": [\"abstract_entity\", \"event\"],\n",
    "                    \"Punishment\": [\"action\", \"event\"]\n",
    "                }\n",
    "            },\n",
    "            \"Hunting\": {\n",
    "                \"lexical_units\": [\"hunt\", \"track\", \"pursue\", \"trap\", \"kill\", \"ambush\", \n",
    "                                 \"chase\", \"stalk\", \"capture\", \"snare\"],\n",
    "                \"frame_elements\": {\n",
    "                    \"Hunter\": [\"PERSON\", \"ANIMAL\"],\n",
    "                    \"Quarry\": [\"PERSON\", \"ANIMAL\"],\n",
    "                    \"Instrument\": [\"physical_object\", \"artifact\"],\n",
    "                    \"Location\": [\"GPE\", \"LOCATION\"]\n",
    "                }\n",
    "            },\n",
    "            \"Travel\": {\n",
    "                \"lexical_units\": [\"travel\", \"journey\", \"voyage\", \"sail\", \"depart\", \"explore\", \n",
    "                                 \"roam\", \"wander\", \"trek\", \"embark\", \"navigate\"],\n",
    "                \"frame_elements\": {\n",
    "                    \"Traveler\": [\"PERSON\", \"ORGANIZATION\"],\n",
    "                    \"Source\": [\"GPE\", \"LOCATION\"],\n",
    "                    \"Goal\": [\"GPE\", \"LOCATION\"],\n",
    "                    \"Path\": [\"GPE\", \"LOCATION\"],\n",
    "                    \"Vehicle\": [\"FACILITY\", \"vehicle\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.text = self._load_corpus()\n",
    "        self.sentences = sent_tokenize(self.text)\n",
    "        self.processed_sentences = []\n",
    "        self.frame_instances = []\n",
    "        self.word_frequencies = Counter()\n",
    "        self.bigram_frequencies = Counter()\n",
    "        \n",
    "        if gazetteer_path and os.path.exists(gazetteer_path):\n",
    "            self._load_gazetteer(gazetteer_path)\n",
    "    \n",
    "    def _load_corpus(self):\n",
    "        try:\n",
    "            with open(self.corpus_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Corpus file not found: {self.corpus_path}\")\n",
    "    \n",
    "    def _load_gazetteer(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    parts = line.strip().rsplit(' ', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        name, label = parts\n",
    "                        self.gazetteer[name.lower()] = label.upper()\n",
    "    \n",
    "    def _get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        return wn.NOUN\n",
    "    \n",
    "    def _get_hypernym_paths(self, word, pos_tag):\n",
    "        cache_key = (word.lower(), pos_tag)\n",
    "        if not hasattr(self, '_hypernym_cache'):\n",
    "            self._hypernym_cache = {}\n",
    "        \n",
    "        if cache_key in self._hypernym_cache:\n",
    "            return self._hypernym_cache[cache_key]\n",
    "        \n",
    "        wn_pos = self._get_wordnet_pos(pos_tag)\n",
    "        synsets = wn.synsets(word, pos=wn_pos)\n",
    "        \n",
    "        all_hypernyms = set()\n",
    "        for synset in synsets[:1]:\n",
    "            try:\n",
    "                hypernyms = list(synset.closure(lambda s: s.hypernyms(), depth=5))\n",
    "                all_hypernyms.update([h.name().split('.')[0] for h in hypernyms[:10]])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self._hypernym_cache[cache_key] = all_hypernyms\n",
    "        return all_hypernyms\n",
    "    \n",
    "    def _apply_ner_with_gazetteer(self, tokens, pos_tags):\n",
    "        chunked = ne_chunk(pos_tags, binary=False)\n",
    "        entities = []\n",
    "        \n",
    "        for item in chunked:\n",
    "            if isinstance(item, Tree):\n",
    "                entity_words = [w for w, t in item.leaves()]\n",
    "                entity_text = ' '.join(entity_words)\n",
    "                label = item.label()\n",
    "                \n",
    "                if entity_text.lower() in self.gazetteer:\n",
    "                    label = self.gazetteer[entity_text.lower()]\n",
    "                \n",
    "                entities.append({\n",
    "                    'text': entity_text,\n",
    "                    'label': label,\n",
    "                    'tokens': entity_words\n",
    "                })\n",
    "            else:\n",
    "                word, tag = item\n",
    "                if word.lower() in self.gazetteer:\n",
    "                    label = self.gazetteer[word.lower()]\n",
    "                    entities.append({\n",
    "                        'text': word,\n",
    "                        'label': label,\n",
    "                        'tokens': [word]\n",
    "                    })\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _match_frame_element(self, word, pos, entities, fe_requirements):\n",
    "        for entity in entities:\n",
    "            if word.lower() in entity['text'].lower():\n",
    "                if entity['label'] in fe_requirements:\n",
    "                    return entity['text']\n",
    "        \n",
    "        hypernyms = self._get_hypernym_paths(word, pos)\n",
    "        for req in fe_requirements:\n",
    "            if not req.isupper():\n",
    "                if req in hypernyms or req.replace('_', ' ') in hypernyms:\n",
    "                    return word\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _detect_frame_instance(self, sentence_num, tokens, pos_tags, entities):\n",
    "        if not hasattr(self, '_lu_set'):\n",
    "            self._lu_set = set()\n",
    "            for frame_def in self.frame_definitions.values():\n",
    "                self._lu_set.update(frame_def[\"lexical_units\"])\n",
    "        \n",
    "        for frame_name, frame_def in self.frame_definitions.items():\n",
    "            for i, (word, pos) in enumerate(pos_tags):\n",
    "                word_lower = word.lower()\n",
    "                \n",
    "                if word_lower not in self._lu_set:\n",
    "                    if pos.startswith('V'):\n",
    "                        lemma = self.lemmatizer.lemmatize(word_lower, wn.VERB)\n",
    "                        if lemma not in frame_def[\"lexical_units\"]:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                if word_lower in frame_def[\"lexical_units\"] or \\\n",
    "                   self.lemmatizer.lemmatize(word_lower, self._get_wordnet_pos(pos)) in frame_def[\"lexical_units\"]:\n",
    "                    frame_instance = {\n",
    "                        'sentence_num': sentence_num,\n",
    "                        'frame': frame_name,\n",
    "                        'lexical_unit': word,\n",
    "                        'lu_position': i,\n",
    "                        'elements': {}\n",
    "                    }\n",
    "                    \n",
    "                    context_window = 10\n",
    "                    start = max(0, i - context_window)\n",
    "                    end = min(len(pos_tags), i + context_window)\n",
    "                    \n",
    "                    for fe_name, fe_requirements in frame_def[\"frame_elements\"].items():\n",
    "                        for j in range(start, end):\n",
    "                            if j != i:\n",
    "                                w, p = pos_tags[j]\n",
    "                                match = self._match_frame_element(w, p, entities, fe_requirements)\n",
    "                                if match and fe_name not in frame_instance['elements']:\n",
    "                                    frame_instance['elements'][fe_name] = match\n",
    "                    \n",
    "                    return frame_instance\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _build_language_model(self):\n",
    "        for sent in self.processed_sentences:\n",
    "            words = [w.lower() for w, p in sent['pos_tags'] if w.isalpha() and w.lower() not in self.stopwords]\n",
    "            self.word_frequencies.update(words)\n",
    "            \n",
    "            for i in range(len(words) - 1):\n",
    "                self.bigram_frequencies[(words[i], words[i+1])] += 1\n",
    "    \n",
    "    def process_corpus(self):\n",
    "        print(f\"Processing {len(self.sentences)} sentences...\")\n",
    "        \n",
    "        for sent_num, sentence in enumerate(self.sentences):\n",
    "            if sent_num % 1000 == 0:\n",
    "                print(f\"  Progress: {sent_num}/{len(self.sentences)} sentences\")\n",
    "            \n",
    "            tokens = word_tokenize(sentence)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            entities = self._apply_ner_with_gazetteer(tokens, pos_tags)\n",
    "            \n",
    "            self.processed_sentences.append({\n",
    "                'sentence_num': sent_num,\n",
    "                'text': sentence,\n",
    "                'tokens': tokens,\n",
    "                'pos_tags': pos_tags,\n",
    "                'entities': entities\n",
    "            })\n",
    "            \n",
    "            frame_instance = self._detect_frame_instance(sent_num, tokens, pos_tags, entities)\n",
    "            if frame_instance:\n",
    "                frame_instance['sentence'] = sentence\n",
    "                self.frame_instances.append(frame_instance)\n",
    "        \n",
    "        self._build_language_model()\n",
    "        self._compute_sentiments()\n",
    "        \n",
    "        print(f\"Processing complete\")\n",
    "    \n",
    "    def _compute_sentiments(self):\n",
    "        try:\n",
    "            from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "            sia = SentimentIntensityAnalyzer()\n",
    "            \n",
    "            for instance in self.frame_instances:\n",
    "                scores = sia.polarity_scores(instance['sentence'])\n",
    "                instance['sentiment_score'] = scores['compound']\n",
    "                \n",
    "                if scores['compound'] > 0.05:\n",
    "                    instance['sentiment_label'] = 'positive'\n",
    "                elif scores['compound'] < -0.05:\n",
    "                    instance['sentiment_label'] = 'negative'\n",
    "                else:\n",
    "                    instance['sentiment_label'] = 'neutral'\n",
    "        except:\n",
    "            for instance in self.frame_instances:\n",
    "                instance['sentiment_score'] = 0\n",
    "                instance['sentiment_label'] = 'neutral'\n",
    "    \n",
    "    def display_sample_results(self, num_samples=5):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Examples (showing {min(num_samples, len(self.frame_instances))} of {len(self.frame_instances)} total)\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        for i, instance in enumerate(self.frame_instances[:num_samples]):\n",
    "            print(f\"Example {i+1} (Sentence {instance['sentence_num']})\")\n",
    "            print(f\"FRAME TYPE: {instance['frame']}\")\n",
    "            print(f\"TRIGGER WORD: '{instance['lexical_unit']}'\")\n",
    "            print(f\"\\nSENTENCE:\\n{instance['sentence']}\\n\")\n",
    "            \n",
    "            if instance['elements']:\n",
    "                print(\"DETECTED ROLES:\")\n",
    "                for role, value in instance['elements'].items():\n",
    "                    print(f\"  {role}: {value}\")\n",
    "            else:\n",
    "                print(\"(No frame elements extracted)\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        frame_counts = Counter(inst['frame'] for inst in self.frame_instances)\n",
    "        \n",
    "        stats = {\n",
    "            'total_sentences': len(self.sentences),\n",
    "            'sentences_with_frames': len(self.frame_instances),\n",
    "            'detection_rate_pct': round(len(self.frame_instances) / len(self.sentences) * 100, 2),\n",
    "            'frame_counts': dict(frame_counts)\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def print_summary(self):\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nBook: {os.path.basename(self.corpus_path)}\")\n",
    "        print(f\"Total Sentences: {stats['total_sentences']:,}\")\n",
    "        print(f\"Sentences with Frames Detected: {stats['sentences_with_frames']:,}\")\n",
    "        print(f\"Detection Rate: {stats['detection_rate_pct']}%\")\n",
    "        print(\"\\nBREAKDOWN BY SCENE TYPE:\")\n",
    "        for frame, count in sorted(stats['frame_counts'].items(), key=lambda x: x[1], reverse=True):\n",
    "            pct = round(count / stats['sentences_with_frames'] * 100, 1)\n",
    "            print(f\"  {frame}: {count} instances ({pct}% of all detected frames)\")\n",
    "    \n",
    "    def print_sentiment_analysis(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SENTIMENT ANALYSIS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        sentiment_by_frame = defaultdict(list)\n",
    "        for instance in self.frame_instances:\n",
    "            sentiment_by_frame[instance['frame']].append(instance['sentiment_score'])\n",
    "        \n",
    "        for frame in ['Revenge', 'Hunting', 'Travel']:\n",
    "            if frame in sentiment_by_frame:\n",
    "                scores = sentiment_by_frame[frame]\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                \n",
    "                pos_count = sum(1 for s in scores if s > 0.05)\n",
    "                neg_count = sum(1 for s in scores if s < -0.05)\n",
    "                neu_count = len(scores) - pos_count - neg_count\n",
    "                \n",
    "                print(f\"{frame} scenes:\")\n",
    "                print(f\"  Average sentiment: {avg_score:.3f}\")\n",
    "                print(f\"  Positive: {pos_count}, Negative: {neg_count}, Neutral: {neu_count}\")\n",
    "                print()\n",
    "    \n",
    "    def print_temporal_distribution(self):\n",
    "        print(\"=\"*80)\n",
    "        print(\"TEMPORAL DISTRIBUTION\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        total = len(self.sentences)\n",
    "        chunk_size = total // 10\n",
    "        \n",
    "        distribution = {f: [0]*10 for f in ['Revenge', 'Hunting', 'Travel']}\n",
    "        \n",
    "        for instance in self.frame_instances:\n",
    "            chunk = min(instance['sentence_num'] // chunk_size, 9)\n",
    "            distribution[instance['frame']][chunk] += 1\n",
    "        \n",
    "        print(\"Frame occurrences across the book (divided into 10 equal segments):\\n\")\n",
    "        \n",
    "        for frame, counts in distribution.items():\n",
    "            total_frame = sum(counts)\n",
    "            if total_frame > 0:\n",
    "                print(f\"{frame}:\")\n",
    "                bars = ['|' + '*' * (count * 40 // max(counts)) if count > 0 else '|' for count in counts]\n",
    "                for i, (count, bar) in enumerate(zip(counts, bars)):\n",
    "                    print(f\"  Section {i+1:2d}: {bar} {count}\")\n",
    "                print()\n",
    "    \n",
    "    def print_vocabulary_analysis(self):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"VOCABULARY ANALYSIS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        frame_texts = defaultdict(list)\n",
    "        for instance in self.frame_instances:\n",
    "            frame_texts[instance['frame']].append(instance['sentence'])\n",
    "        \n",
    "        print(\"Most distinctive words by frame type:\\n\")\n",
    "        \n",
    "        for frame in ['Revenge', 'Hunting', 'Travel']:\n",
    "            if frame in frame_texts and len(frame_texts[frame]) > 1:\n",
    "                vectorizer = TfidfVectorizer(max_features=8, stop_words='english')\n",
    "                try:\n",
    "                    tfidf = vectorizer.fit_transform(frame_texts[frame])\n",
    "                    words = vectorizer.get_feature_names_out()\n",
    "                    print(f\"{frame}: {', '.join(words)}\")\n",
    "                except:\n",
    "                    print(f\"{frame}: insufficient data\")\n",
    "    \n",
    "    def print_word_frequency_analysis(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"WORD FREQUENCY ANALYSIS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        top_words = self.word_frequencies.most_common(20)\n",
    "        \n",
    "        print(\"Most frequent words in the text:\\n\")\n",
    "        \n",
    "        max_count = top_words[0][1] if top_words else 1\n",
    "        \n",
    "        for word, count in top_words:\n",
    "            bar_length = int((count / max_count) * 40)\n",
    "            bar = '=' * bar_length\n",
    "            print(f\"  {word:15s} {bar} {count:,}\")\n",
    "        \n",
    "        total_words = sum(self.word_frequencies.values())\n",
    "        unique_words = len(self.word_frequencies)\n",
    "        \n",
    "        print(f\"\\nTotal words: {total_words:,}\")\n",
    "        print(f\"Unique words: {unique_words:,}\")\n",
    "        print(f\"Vocabulary richness: {unique_words/total_words:.4f}\")\n",
    "    \n",
    "    def generate_random_sentence(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PROBABILISTIC SENTENCE GENERATION\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        if not self.bigram_frequencies:\n",
    "            print(\"No language model available\")\n",
    "            return\n",
    "        \n",
    "        sentence_words = []\n",
    "        \n",
    "        all_first_words = [w1 for w1, w2 in self.bigram_frequencies.keys()]\n",
    "        current = random.choice(all_first_words)\n",
    "        sentence_words.append(current)\n",
    "        \n",
    "        for _ in range(random.randint(8, 15)):\n",
    "            next_options = [(w2, count) for (w1, w2), count in self.bigram_frequencies.items() if w1 == current]\n",
    "            \n",
    "            if not next_options:\n",
    "                break\n",
    "            \n",
    "            total = sum(count for _, count in next_options)\n",
    "            rand_val = random.uniform(0, total)\n",
    "            \n",
    "            cumsum = 0\n",
    "            for next_word, count in next_options:\n",
    "                cumsum += count\n",
    "                if rand_val <= cumsum:\n",
    "                    current = next_word\n",
    "                    sentence_words.append(current)\n",
    "                    break\n",
    "        \n",
    "        generated = ' '.join(sentence_words).capitalize() + '.'\n",
    "        \n",
    "        print(\"Generated sentence based on bigram probabilities however may not follow all grammar rules due to this:\\n\")\n",
    "        print(f'\"{generated}\"')\n",
    "    \n",
    "    def export_to_csv(self, filename='frame_detections.csv'):\n",
    "        data = []\n",
    "        for instance in self.frame_instances:\n",
    "            row = {\n",
    "                'Sentence_Number': instance['sentence_num'],\n",
    "                'Frame_Type': instance['frame'],\n",
    "                'Trigger_Word': instance['lexical_unit'],\n",
    "                'Sentiment': instance['sentiment_label'],\n",
    "                'Sentiment_Score': instance['sentiment_score'],\n",
    "                'Full_Sentence': instance['sentence']\n",
    "            }\n",
    "            \n",
    "            for role in ['Hunter', 'Quarry', 'Instrument', 'Location', \n",
    "                        'Avenger', 'Offender', 'Injury', 'Punishment',\n",
    "                        'Traveler', 'Source', 'Goal', 'Path', 'Vehicle']:\n",
    "                row[role] = instance['elements'].get(role, '')\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nExported {len(df)} detections to '{filename}'\")\n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LITERARY SEMANTIC FRAME ANALYZER\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    analyzer = LiteraryFrameAnalyzer(\n",
    "        corpus_path='Moby Dick.txt',\n",
    "        gazetteer_path='gazeteer2.0'\n",
    "    )\n",
    "    \n",
    "    analyzer.process_corpus()\n",
    "    \n",
    "    analyzer.print_summary()\n",
    "    \n",
    "    analyzer.print_sentiment_analysis()\n",
    "    \n",
    "    analyzer.print_temporal_distribution()\n",
    "    \n",
    "    analyzer.print_vocabulary_analysis()\n",
    "    \n",
    "    analyzer.print_word_frequency_analysis()\n",
    "    \n",
    "    analyzer.generate_random_sentence()\n",
    "    \n",
    "    analyzer.display_sample_results(num_samples=20)\n",
    "    \n",
    "    df = analyzer.export_to_csv('moby_dick_frame_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9882978-e71c-4931-8f5c-6f4338e2fc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
